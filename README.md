# Multi-Modal-Household-Instruction-Dataset
In this repository, we provide code for the data-capturing application we developed in tkinter, the model scripts, utility files and links to the datasets.

## Introduction
This repository contains the code and links to the datasets used in a report titled,  "Multi-modal Few-shot Learning in a Household Robot". In the project a multi-modal household robotic system was developed which allows a user to interact with common household objects in a language of their choice by giving a robot a simple instruction to complete. The system was designed to perform well on lower-resource languages. As part of this project, we also collected a dataset of paired images and speech recordings where the verbal instruction is invoked by the user being shown a visual stimulus. The links to the speech and vision datasets are added later on. 

## Description
### bound_box_test

### Files beginning with "complete"

### generate_questions

### lable_audio_as_tuple

### complete_utils

### interview_gui_1

### class_mappings_optimized

## Dataset and Model Weights
The vision dataset and weights of the top 5 YOLO models implemented in this project can be found at: https://drive.google.com/drive/folders/10hJtw0Jq2xcDIj-gWoOCALfQXy7sF56H?usp=share_link

The speech dataset is currently in the process of being published. Once this is complete a link to access it will be placed here.

## Environment Setup
To setup the environment for the object detector refer to: https://github.com/The-Wubster/Skripsie.git

## License

Copyright (c) 2022 Stellenbosch University

This data is released under a Creative Commons Attribution-ShareAlike 
license
(<http://creativecommons.org/licenses/by-sa/4.0/>).
